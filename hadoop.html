<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BDA Portfolio - Hadoop</title>
    
    <style>
        body { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif; margin: 0; padding: 0; background-color: #f4f7f6; color: #333; line-height: 1.6; }
        .container { max-width: 960px; margin: 20px auto; padding: 0 20px; }
        header { background-color: #0a4a6e; color: #ffffff; padding: 40px 20px; text-align: center; border-radius: 8px 8px 0 0; }
        header h1 { margin: 0; font-size: 2.5em; }
        header p { font-size: 1.2em; opacity: 0.9; margin-top: 10px; }
        nav { background-color: #333; overflow: hidden; }
        nav ul { list-style-type: none; margin: 0; padding: 0; display: flex; flex-wrap: wrap; justify-content: center; }
        nav li { flex-grow: 1; }
        nav a { display: block; color: white; text-align: center; padding: 14px 16px; text-decoration: none; font-size: 0.95em; transition: background-color 0.3s; }
        nav a:hover { background-color: #575757; }
        nav a.active { background-color: #007bff; font-weight: bold; }
        main { background-color: #ffffff; padding: 30px; border-radius: 0 0 8px 8px; box-shadow: 0 4px 12px rgba(0,0,0,0.05); }
        section { margin-bottom: 30px; }
        section h2 { color: #0a4a6e; border-bottom: 2px solid #f0f0f0; padding-bottom: 10px; margin-bottom: 20px; }
        section h3 { color: #005a9c; }
        pre { background-color: #2b2b2b; color: #f8f8f2; padding: 15px; border-radius: 5px; overflow-x: auto; font-family: 'Courier New', Courier, monospace; font-size: 0.9em; }
        code { font-family: 'Courier New', Courier, monospace; }
        :not(pre) > code { background-color: #eee; padding: 2px 5px; border-radius: 3px; color: #c7254e; }
        footer { text-align: center; margin-top: 30px; padding: 20px; font-size: 0.9em; color: #777; }
    </style>
</head>
<body>

    <header>
        <h1>Hadoop & MapReduce</h1>
        <p>Installation Concepts and the Word Count Example</p>
    </header>

    <div class="container">
        <nav>
            <ul>
                <li><a href="index.html">Home</a></li>
                <li><a href="hadoop.html" class="active">1. Hadoop</a></li>
                <li><a href="pig.html">2. Apache Pig</a></li>
                <li><a href="hive.html">3. Apache Hive</a></li>
                <li><a href="mongodb.html">4. MongoDB</a></li>
            </ul>
        </nav>

        <main>
            <section id="installation">
                <h2>Hadoop Installation (Conceptual Steps)</h2>
                <p>Installing Hadoop (especially in a pseudo-distributed mode) involves several key configuration steps. Here is a high-level overview of the process:</p>
                <ol>
                    <li><strong>Download & Unpack:</strong> Download the Hadoop binary tarball and unpack it to a directory (e.g., <code>/usr/local/hadoop</code>).</li>
                    <li><strong>Set Environment Variables:</strong> Set <code>JAVA_HOME</code> and <code>HADOOP_HOME</code> in your <code>.bashrc</code> file.</li>
                    <li><strong>Configure <code>core-site.xml</code>:</strong> Specify the default file system (HDFS) and the NameNode's URI.
<pre><code>&lt;configuration&gt;
    &lt;property&gt;
        &lt;name&gt;fs.defaultFS&lt;/name&gt;
        &lt;value&gt;hdfs://localhost:9000&lt;/value&gt;
    &lt;/property&gt;
&lt;/configuration&gt;</code></pre>
                    </li>
                    <li><strong>Configure <code>hdfs-site.xml</code>:</strong> Set the replication factor (1 for pseudo-distributed) and paths for NameNode and DataNode data.
<pre><code>&lt;configuration&gt;
    &lt;property&gt;
        &lt;name&gt;dfs.replication&lt;/name&gt;
        &lt;value&gt;1&lt;/value&gt;
    &lt;/property&gt;
&lt;/configuration&gt;</code></pre>
                    </li>
                    <li><strong>Configure <code>mapred-site.xml</code>:</strong> (May need to rename from template) Specify YARN as the MapReduce framework.
<pre><code>&lt;configuration&gt;
    &lt;property&gt;
        &lt;name&gt;mapreduce.framework.name&lt;/name&gt;
        &lt;value&gt;yarn&lt;/value&gt;
    &lt;/property&gt;
&lt;/configuration&gt;</code></pre>
                    </li>
                    <li><strong>Configure <code>yarn-site.xml</code>:</strong> Configure the NodeManager and ResourceManager services.</li>
                    <li><strong>Format NameNode:</strong> Run <code>hdfs namenode -format</code>. This initializes the HDFS metadata.</li>
                    <li><strong>Start Services:</strong> Use <code>start-dfs.sh</code> and <code>start-yarn.sh</code> to launch the HDFS and YARN daemons.</li>
                </ol>
            </section>

            <section id="wordcount">
                <h2>MapReduce Word Count Example</h2>
                <p>The "Hello, World!" of MapReduce. This program reads text files and counts how often each word appears.</p>

                <h3>WordCountMapper.java</h3>
<pre><code>import java.io.IOException;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;

public class WordCountMapper extends Mapper&lt;LongWritable, Text, Text, IntWritable&gt; {
    private final static IntWritable one = new IntWritable(1);
    private Text word = new Text();

    public void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
        String line = value.toString();
        String[] words = line.split("\\s+"); // Split by whitespace
        for (String str : words) {
            word.set(str.toLowerCase()); // Normalize to lowercase
            context.write(word, one); // Emit (word, 1)
        }
    }
}</code></pre>

                <h3>WordCountReducer.java</h3>
<pre><code>import java.io.IOException;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Reducer;

public class WordCountReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt; {
    private IntWritable result = new IntWritable();

    public void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException {
        int sum = 0;
        for (IntWritable val : values) {
            sum += val.get(); // Sum up all the 1s for this word
        }
        result.set(sum);
        context.write(key, result); // Emit (word, total_count)
    }
}</code></pre>

                <h3>WordCountDriver.java (Main Class)</h3>
<pre><code>import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

public class WordCountDriver {
    public static void main(String[] args) throws Exception {
        if (args.length != 2) {
            System.err.println("Usage: WordCountDriver &lt;input path&gt; &lt;output path&gt;");
            System.exit(-1);
        }
        
        Job job = Job.getInstance();
        job.setJarByClass(WordCountDriver.class);
        job.setJobName("Word Count");

        FileInputFormat.addInputPath(job, new Path(args[0]));
        FileOutputFormat.setOutputPath(job, new Path(args[1]));

        job.setMapperClass(WordCountMapper.class);
        job.setReducerClass(WordCountReducer.class);

        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(IntWritable.class);

        System.exit(job.waitForCompletion(true) ? 0 : 1);
    }
}</code></pre>

                <h3>Running the Job</h3>
<pre><code># 1. Compile your Java files and create a JAR (e.g., wordcount.jar)
# 2. Put your input text file(s) into HDFS
hdfs dfs -mkdir /input
hdfs dfs -put my_text_file.txt /input

# 3. Run the MapReduce job
hadoop jar wordcount.jar WordCountDriver /input /output

# 4. Check the results
hdfs dfs -cat /output/part-r-00000</code></pre>
            </section>
        </main>
    </div>

    <footer>
        <p>&copy; 2025 [Your Name Here] - All rights reserved.</p>
    </footer>

</body>

</html>
